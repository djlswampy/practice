{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82653b5e-3e64-467b-bee0-36516f8bb5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response(id='resp_076f734ea6ed70f00068dc0e103e18819493388269bb4e15e3', created_at=1759251984.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-5-nano-2025-08-07', object='response', output=[ResponseReasoningItem(id='rs_076f734ea6ed70f00068dc0e10ea648194973abaea66dd55fb', summary=[], type='reasoning', content=None, encrypted_content=None, status=None), ResponseOutputMessage(id='msg_076f734ea6ed70f00068dc0e16b4fc8194acf7c922e3448dca', content=[ResponseOutputText(annotations=[], text='ì˜›ë‚  ì•„ì£¼ ê³ ìš´ ìˆ² ì†ì— ì€ë¹› ë¿”ì„ ê°€ì§„ ì‘ì€ ìœ ë‹ˆì½˜ì´ ì‚´ì•˜ì–´ìš”. ë‹¬ë¹›ì´ ë¹„ì¶”ëŠ” ë°¤ë§ˆë‹¤ ê·¸ëŠ” ë‹¤ì •í•œ ì†ì‚­ì„ìœ¼ë¡œ ê¸¸ ìƒì€ ì´ë“¤ì˜ ê¿ˆì„ ì°¾ì•„ ì£¼ì—ˆì–´ìš”. ì•„ì´ë“¤ì´ ì°½ë¬¸ì„ ë‹«ê³  ì ë“¤ë©´, ìœ ë‹ˆì½˜ì€ ë³„ë¹›ìœ¼ë¡œ ê¸¸ì„ ë¹„ì¶° ì£¼ë©° í¸ì•ˆí•œ ì ì„ ì„ ë¬¼í•œë‹µë‹ˆë‹¤.', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort='medium', generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=ResponseUsage(input_tokens=18, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=1249, output_tokens_details=OutputTokensDetails(reasoning_tokens=1152), total_tokens=1267), user=None, billing={'payer': 'developer'}, store=True)\n"
     ]
    }
   ],
   "source": [
    "# OpenAI clientë¥¼ í†µí•´ llm í˜¸ì¶œ\n",
    "\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.responses.create(\n",
    "  model=\"gpt-5-nano\",\n",
    "  input=\"Tell me a three sentence bedtime story about a unicorn in korean\"\n",
    ")\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75436d37-35e1-4867-a505-b9a34dfbabdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë­ì²´ì¸ì—ì„œ ì œê³µí•˜ëŠ” ì˜¤í”ˆAI LLM ë˜í¼ë¥¼ í†µí•œ llm í˜¸ì¶œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b06ea18c-376d-45fe-8d9a-b8d71ca60b77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' ì„œìš¸ ì…ë‹ˆë‹¤\\n\\nì„œìš¸ì€ í•œêµ­ì˜ ì •ì¹˜, ê²½ì œ, ë¬¸í™” ì¤‘ì‹¬ì§€ë¡œì„œ ë§ì€ ì¸êµ¬ì™€ ì—­ì‚¬ì ì¸ ê°€ì¹˜ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. í•œêµ­ì˜ ì£¼ìš” ê´€ê´‘ì§€ ì¤‘ í•˜ë‚˜ì´ë©°, ì „ ì„¸ê³„ì—ì„œë„ ìœ ëª…í•œ ë„ì‹œì…ë‹ˆë‹¤. ì„œìš¸ì—ëŠ” í•œêµ­ì˜ ì „í†µê³¼ í˜„ëŒ€ê°€ ê³µì¡´í•˜ëŠ” ë‹¤ì–‘í•œ ê´€ê´‘ì§€, ë¨¹ê±°ë¦¬, ì‡¼í•‘ ëª…ì†Œê°€ ìˆì–´ì„œ ë§¤ë…„ ë§ì€ ê´€ê´‘ê°ë“¤ì´ ì°¾ì•„ì˜¤ê³  ìˆìŠµë‹ˆë‹¤. ë˜í•œ ì„œìš¸ì€ êµ­ì œì ì¸ ë„ì‹œë¡œì„œ ì—¬ëŸ¬ ë‚˜ë¼ì™€ì˜ êµë¥˜ì™€ ë¬¸í™”ì  ë‹¤ì–‘ì„±ì„ ê²½í—˜í•  ìˆ˜ ìˆëŠ” ê³³ì…ë‹ˆë‹¤. í•œêµ­ì˜ ìˆ˜ë„ì¸ ì„œìš¸ì€ ë¹ ë¥´ê²Œ ë°œì „í•˜ê³  ë³€í™”í•˜ëŠ” ëª¨ìŠµì„ ë³´ì—¬ì£¼ë©°, ì „ ì„¸ê³„ì—ì„œë„ ì£¼ëª©ë°›ëŠ” ë„ì‹œê°€ ë˜ê³ '"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ê¸°ë³¸ LLM í˜¸ì¶œ\n",
    "\n",
    "from langchain_openai.llms import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "model = OpenAI(model = 'gpt-3.5-turbo-instruct')\n",
    "\n",
    "model.invoke('í•œêµ­ì˜ ìˆ˜ë„ëŠ” ')\n",
    "# print(\"result: \" + result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "94873dab-eb87-4c5c-b497-c86e50767160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 12, 'total_tokens': 33, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CLYcAQm6nmTX9y7mB2BB4rubmlo6a', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--16d03d2b-1f14-4c8d-ab62-5b4108b60ca0-0', usage_metadata={'input_tokens': 12, 'output_tokens': 21, 'total_tokens': 33, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chat model í˜¸ì¶œ\n",
    "\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "model = ChatOpenAI() # gpt-3.5-turbo-0125 (ChatOpenAIì˜ ê¸°ë³¸ ëª¨ë¸) ì‚¬ìš©ì´í•´\n",
    "prompt = [HumanMessage('í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?')]\n",
    "model.invoke(prompt)\n",
    "\n",
    "# print(\"result: \" + result) ---> error. resultì˜ íƒ€ì…ì´ strì´ ì•„ë‹ˆë¼ AIMessage ê°ì²´ì„\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4d771c39-e0b6-40d8-888b-67e28de6d697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”? ğŸ™‚', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 12, 'total_tokens': 34, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CLYdCni4liWHrbhmSqZD7bVgAsGu9', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--c920ae64-078b-4a8a-929a-63ff9c2ee4b9-0', usage_metadata={'input_tokens': 12, 'output_tokens': 22, 'total_tokens': 34, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# strì„ ë„£ìœ¼ë©´ LangChainì´ ìë™ìœ¼ë¡œ HumanMessageë¡œ ë³€í™˜\n",
    "model.invoke(\"ì•ˆë…•í•˜ì„¸ìš”\")\n",
    "# ë‚´ë¶€ì ìœ¼ë¡œ ì´ë ‡ê²Œ ë³€í™˜ë¨ â†“\n",
    "# model.invoke([HumanMessage(\"ì•ˆë…•í•˜ì„¸ìš”\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "211ace22-dece-4ae8-978b-af401671163b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì„œìš¸ì…ë‹ˆë‹¤!!!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "model = ChatOpenAI()\n",
    "system_msg = SystemMessage(\n",
    "    '''ë‹¹ì‹ ì€ ë¬¸ì¥ ëì— ëŠë‚Œí‘œ ì„¸ ê°œë¥¼ ë¶™ì—¬ ëŒ€ë‹µí•˜ëŠ” ì¹œì ˆí•œ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.'''\n",
    ")\n",
    "human_msg = HumanMessage('í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?')\n",
    "\n",
    "ai_msg = model.invoke([system_msg, human_msg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "877371ab-0b04-4d73-a0a8-a244142210de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='ì•„ë˜ ì‘ì„±í•œ ì»¨í…ìŠ¤íŠ¸(Context)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ\\n    ì§ˆë¬¸(Question)ì— ëŒ€ë‹µí•˜ì„¸ìš”. ì œê³µëœ ì •ë³´ë¡œ ëŒ€ë‹µí•  ìˆ˜ ì—†ëŠ” ì§ˆë¬¸ì´ë¼ë©´ \"ëª¨ë¥´ê² ì–´ìš”.\" ë¼ê³  ë‹µí•˜ì„¸ìš”.\\n\\nContext: ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸(LLM)ì€ ìì—°ì–´ ì²˜ë¦¬(NLP) ë¶„ì•¼ì˜ ìµœì‹  ë°œì „ì„ ì´ëŒê³  ìˆìŠµë‹ˆë‹¤.\\n    ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸ì€ ë” ì‘ì€ ëª¨ë¸ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì´ë©°, NLP ê¸°ëŠ¥ì„ ê°–ì¶˜ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ê°œë°œí•˜ëŠ” ê°œë°œìë“¤ì—ê²Œ\\n    ë§¤ìš° ì¤‘ìš”í•œ ë„êµ¬ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤. ê°œë°œìë“¤ì€ Hugging Faceì˜ `transformers` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼\\n    í™œìš©í•˜ê±°ë‚˜, `openai` ë° `cohere` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í†µí•´ OpenAIì™€ Cohereì˜ ì„œë¹„ìŠ¤ë¥¼ ì´ìš©í•˜ì—¬\\n    ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸ì„ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n    \\n\\nQuestion: ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸ì€ ì–´ë””ì„œ ì œê³µí•˜ë‚˜ìš”?\\n\\nAnswer: ')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  ë™ì  ì…ë ¥ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ë¥¼ ì†ì‰½ê²Œ ìˆ˜ì •í•  ìˆ˜ ìˆëŠ” í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì¸í„°í˜ì´ìŠ¤\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate.from_template('''ì•„ë˜ ì‘ì„±í•œ ì»¨í…ìŠ¤íŠ¸(Context)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ\n",
    "    ì§ˆë¬¸(Question)ì— ëŒ€ë‹µí•˜ì„¸ìš”. ì œê³µëœ ì •ë³´ë¡œ ëŒ€ë‹µí•  ìˆ˜ ì—†ëŠ” ì§ˆë¬¸ì´ë¼ë©´ \"ëª¨ë¥´ê² ì–´ìš”.\" ë¼ê³  ë‹µí•˜ì„¸ìš”.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: ''')\n",
    "\n",
    "template.invoke({\n",
    "    'context': '''ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸(LLM)ì€ ìì—°ì–´ ì²˜ë¦¬(NLP) ë¶„ì•¼ì˜ ìµœì‹  ë°œì „ì„ ì´ëŒê³  ìˆìŠµë‹ˆë‹¤.\n",
    "    ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸ì€ ë” ì‘ì€ ëª¨ë¸ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì´ë©°, NLP ê¸°ëŠ¥ì„ ê°–ì¶˜ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ê°œë°œí•˜ëŠ” ê°œë°œìë“¤ì—ê²Œ\n",
    "    ë§¤ìš° ì¤‘ìš”í•œ ë„êµ¬ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤. ê°œë°œìë“¤ì€ Hugging Faceì˜ `transformers` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼\n",
    "    í™œìš©í•˜ê±°ë‚˜, `openai` ë° `cohere` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í†µí•´ OpenAIì™€ Cohereì˜ ì„œë¹„ìŠ¤ë¥¼ ì´ìš©í•˜ì—¬\n",
    "    ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸ì„ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "    ''',\n",
    "    'question': 'ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸ì€ ì–´ë””ì„œ ì œê³µí•˜ë‚˜ìš”?'\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828f59d8-2998-4483-ac87-f095b91cb3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ë™ì  ì…ë ¥ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ë¥¼ ì†ì‰½ê²Œ ìˆ˜ì •í•  ìˆ˜ ìˆëŠ” í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì¸í„°í˜ì´ìŠ¤\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate.from_template('''ì•„ë˜ ì‘ì„±í•œ ì»¨í…ìŠ¤íŠ¸(Context)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ\n",
    "    ì§ˆë¬¸(Question)ì— ëŒ€ë‹µí•˜ì„¸ìš”. ì œê³µëœ ì •ë³´ë¡œ ëŒ€ë‹µí•  ìˆ˜ ì—†ëŠ” ì§ˆë¬¸ì´ë¼ë©´ \"ëª¨ë¥´ê² ì–´ìš”.\" ë¼ê³  ë‹µí•˜ì„¸ìš”.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: ''')\n",
    "\n",
    "prompt = template.invoke({\n",
    "    'context': '''ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸(LLM)ì€ ìì—°ì–´ ì²˜ë¦¬(NLP) ë¶„ì•¼ì˜ ìµœì‹  ë°œì „ì„ ì´ëŒê³  ìˆìŠµë‹ˆë‹¤.\n",
    "    ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸ì€ ë” ì‘ì€ ëª¨ë¸ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì´ë©°, NLP ê¸°ëŠ¥ì„ ê°–ì¶˜ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ê°œë°œí•˜ëŠ” ê°œë°œìë“¤ì—ê²Œ\n",
    "    ë§¤ìš° ì¤‘ìš”í•œ ë„êµ¬ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤. ê°œë°œìë“¤ì€ Hugging Faceì˜ `transformers` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼\n",
    "    í™œìš©í•˜ê±°ë‚˜, `openai` ë° `cohere` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í†µí•´ OpenAIì™€ Cohereì˜ ì„œë¹„ìŠ¤ë¥¼ ì´ìš©í•˜ì—¬\n",
    "    ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸ì„ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "    ''',\n",
    "    'question': 'ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸ì€ ì–´ë””ì„œ ì œê³µí•˜ë‚˜ìš”?'\n",
    "})\n",
    "\n",
    "model = OpenAI()\n",
    "model.invoke(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a1c3f332-c161-45dc-bb02-fd54335193ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nê¹€ì² ìˆ˜'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate.from_template('''Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Answer: ''')\n",
    "\n",
    "prompt = template.invoke({\n",
    "    'context': 'ë‚´ì´ë¦„ì€ ê¹€ì² ìˆ˜!',\n",
    "    'question': 'ë‚´ ì´ë¦„ì´ ë­ë¼ê³ ??'\n",
    "})\n",
    "\n",
    "model = OpenAI()\n",
    "model.invoke(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66a19684-cada-4d14-b727-6b0c0c7ff501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='ì•„ë˜ ì‘ì„±í•œ ì»¨í…ìŠ¤íŠ¸(Context)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ\\n    ì§ˆë¬¸(Question)ì— ëŒ€ë‹µí•˜ì„¸ìš”. ì œê³µëœ ì •ë³´ë¡œ ëŒ€ë‹µí•  ìˆ˜ ì—†ëŠ” ì§ˆë¬¸ì´ë¼ë©´ \"ëª¨ë¥´ê² ì–´ìš”.\" ë¼ê³  ë‹µí•˜ì„¸ìš”.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Context: ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸(LLM)ì€ ìì—°ì–´ ì²˜ë¦¬(NLP) ë¶„ì•¼ì˜ ìµœì‹  ë°œì „ì„ ì´ëŒê³  ìˆìŠµë‹ˆë‹¤.\\n    ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸ì€ ë” ì‘ì€ ëª¨ë¸ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì´ë©°, NLP ê¸°ëŠ¥ì„ ê°–ì¶˜ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ê°œë°œí•˜ëŠ” ê°œë°œìë“¤ì—ê²Œ\\n    ë§¤ìš° ì¤‘ìš”í•œ ë„êµ¬ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤. ê°œë°œìë“¤ì€ Hugging Faceì˜ `transformers` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼\\n    í™œìš©í•˜ê±°ë‚˜, `openai` ë° `cohere` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í†µí•´ OpenAIì™€ Cohereì˜ ì„œë¹„ìŠ¤ë¥¼ ì´ìš©í•˜ì—¬\\n    ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸ì„ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n    ', additional_kwargs={}, response_metadata={}), HumanMessage(content='Question: ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸ì€ ì–´ë””ì„œ ì œê³µí•˜ë‚˜ìš”?', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    ('system', '''ì•„ë˜ ì‘ì„±í•œ ì»¨í…ìŠ¤íŠ¸(Context)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ\n",
    "    ì§ˆë¬¸(Question)ì— ëŒ€ë‹µí•˜ì„¸ìš”. ì œê³µëœ ì •ë³´ë¡œ ëŒ€ë‹µí•  ìˆ˜ ì—†ëŠ” ì§ˆë¬¸ì´ë¼ë©´ \"ëª¨ë¥´ê² ì–´ìš”.\" ë¼ê³  ë‹µí•˜ì„¸ìš”.'''),\n",
    "    ('human', 'Context: {context}'),\n",
    "    ('human', 'Question: {question}'),\n",
    "])\n",
    "\n",
    "template.invoke({\n",
    "    'context': '''ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸(LLM)ì€ ìì—°ì–´ ì²˜ë¦¬(NLP) ë¶„ì•¼ì˜ ìµœì‹  ë°œì „ì„ ì´ëŒê³  ìˆìŠµë‹ˆë‹¤.\n",
    "    ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸ì€ ë” ì‘ì€ ëª¨ë¸ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì´ë©°, NLP ê¸°ëŠ¥ì„ ê°–ì¶˜ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ê°œë°œí•˜ëŠ” ê°œë°œìë“¤ì—ê²Œ\n",
    "    ë§¤ìš° ì¤‘ìš”í•œ ë„êµ¬ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤. ê°œë°œìë“¤ì€ Hugging Faceì˜ `transformers` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼\n",
    "    í™œìš©í•˜ê±°ë‚˜, `openai` ë° `cohere` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í†µí•´ OpenAIì™€ Cohereì˜ ì„œë¹„ìŠ¤ë¥¼ ì´ìš©í•˜ì—¬\n",
    "    ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸ì„ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "    ''',\n",
    "    'question': 'ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸ì€ ì–´ë””ì„œ ì œê³µí•˜ë‚˜ìš”?'\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59b21c5f-cb43-4b7d-8376-eecbadaca1da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸ì€ Hugging Faceì˜ `transformers` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í†µí•´ í™œìš©í•˜ê±°ë‚˜, OpenAI ë° Cohereì˜ ì„œë¹„ìŠ¤ë¥¼ ì´ìš©í•˜ì—¬ ì œê³µë©ë‹ˆë‹¤.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 54, 'prompt_tokens': 310, 'total_tokens': 364, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CLyj0vszCBGpUU5Y0k9SceHqmEPWs', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--71e0b6e3-638a-4343-89b7-0ea29e35b8ba-0', usage_metadata={'input_tokens': 310, 'output_tokens': 54, 'total_tokens': 364, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "# `template`ê³¼ `model`ì€ ì–¸ì œë“  ë‹¤ì‹œ ì“¸ ìˆ˜ ìˆë‹¤\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    ('system', '''ì•„ë˜ ì‘ì„±í•œ ì»¨í…ìŠ¤íŠ¸(Context)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ\n",
    "    ì§ˆë¬¸(Question)ì— ëŒ€ë‹µí•˜ì„¸ìš”. ì œê³µëœ ì •ë³´ë¡œ ëŒ€ë‹µí•  ìˆ˜ ì—†ëŠ” ì§ˆë¬¸ì´ë¼ë©´ \"ëª¨ë¥´ê² ì–´ìš”.\" ë¼ê³  ë‹µí•˜ì„¸ìš”.'''),\n",
    "    ('human', 'Context: {context}'),\n",
    "    ('human', 'Question: {question}'),\n",
    "])\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "# `prompt`ì™€ `completion`ì€ templateê³¼ modelì„ ì‹¤í–‰í•œ ê²°ê³¼ë‹¤\n",
    "\n",
    "prompt = template.invoke({\n",
    "    'context': '''ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸(LLM)ì€ ìì—°ì–´ ì²˜ë¦¬(NLP) ë¶„ì•¼ì˜ ìµœì‹  ë°œì „ì„ ì´ëŒê³  ìˆìŠµë‹ˆë‹¤.\n",
    "    ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸ì€ ë” ì‘ì€ ëª¨ë¸ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì´ë©°, NLP ê¸°ëŠ¥ì„ ê°–ì¶˜ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ê°œë°œí•˜ëŠ” ê°œë°œìë“¤ì—ê²Œ\n",
    "    ë§¤ìš° ì¤‘ìš”í•œ ë„êµ¬ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤. ê°œë°œìë“¤ì€ Hugging Faceì˜ `transformers` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼\n",
    "    í™œìš©í•˜ê±°ë‚˜, `openai` ë° `cohere` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í†µí•´ OpenAIì™€ Cohereì˜ ì„œë¹„ìŠ¤ë¥¼ ì´ìš©í•˜ì—¬\n",
    "    ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸ì„ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "    ''',\n",
    "    'question': 'ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸ì€ ì–´ë””ì„œ ì œê³µí•˜ë‚˜ìš”?'\n",
    "})\n",
    "\n",
    "model.invoke(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c88d71f-d965-4806-8fb4-aaae1269641c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"answer\":\"ë‘˜ ë‹¤ ê°™ë‹¤.\",\"justification\":\"1kgì˜ ì§ˆëŸ‰ì€ ë™ì¼í•˜ë¯€ë¡œ ì§€êµ¬ì—ì„œ ì¸¡ì •í•˜ëŠ” ë¬´ê²Œë„ ê°™ë‹¤. ë‹¤ë§Œ ë¶€í”¼ì™€ ë°€ë„ëŠ” ë‹¤ë¥´ë‹¤.\"}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "class AnswerWithJustification(BaseModel):\n",
    "    '''ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ê³¼ ê·¸ì— ëŒ€í•œ ê·¼ê±°(justification)ë¥¼ í•¨ê»˜ ì œê³µí•˜ì„¸ìš”.'''\n",
    "    answer: str\n",
    "    '''ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€'''\n",
    "    justification: str\n",
    "    '''ë‹µë³€ì— ëŒ€í•œ ê·¼ê±°'''\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-5-nano', temperature=0)\n",
    "structured_llm = llm.with_structured_output(AnswerWithJustification)\n",
    "\n",
    "result = structured_llm.invoke('''1 í‚¬ë¡œê·¸ë¨ì˜ ë²½ëŒê³¼ 1 í‚¬ë¡œê·¸ë¨ì˜ ê¹ƒí„¸ ì¤‘ ì–´ëŠ ìª½ì´ ë” ë¬´ê²ë‚˜ìš”?''')\n",
    "\n",
    "print(result.model_dump_json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87ad447e-d30e-4dae-a75b-8427603f5c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple', 'banana', 'cherry']\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "parser = CommaSeparatedListOutputParser()\n",
    "items = parser.invoke(\"apple, banana, cherry\")\n",
    "print(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f7a285b-be04-4c0a-80d4-e5b48e162f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' additional_kwargs={} response_metadata={} id='run--87502fc0-37b5-46d0-9778-f0e7f794a6c9'\n",
      "content='ì•ˆ' additional_kwargs={} response_metadata={} id='run--87502fc0-37b5-46d0-9778-f0e7f794a6c9'\n",
      "content='ë…•' additional_kwargs={} response_metadata={} id='run--87502fc0-37b5-46d0-9778-f0e7f794a6c9'\n",
      "content='í•˜ì„¸ìš”' additional_kwargs={} response_metadata={} id='run--87502fc0-37b5-46d0-9778-f0e7f794a6c9'\n",
      "content='!' additional_kwargs={} response_metadata={} id='run--87502fc0-37b5-46d0-9778-f0e7f794a6c9'\n",
      "content=' ë§Œ' additional_kwargs={} response_metadata={} id='run--87502fc0-37b5-46d0-9778-f0e7f794a6c9'\n",
      "content='ì•½' additional_kwargs={} response_metadata={} id='run--87502fc0-37b5-46d0-9778-f0e7f794a6c9'\n",
      "content=' ë”' additional_kwargs={} response_metadata={} id='run--87502fc0-37b5-46d0-9778-f0e7f794a6c9'\n",
      "content=' ë„' additional_kwargs={} response_metadata={} id='run--87502fc0-37b5-46d0-9778-f0e7f794a6c9'\n",
      "content='ì›€' additional_kwargs={} response_metadata={} id='run--87502fc0-37b5-46d0-9778-f0e7f794a6c9'\n",
      "content='ì´' additional_kwargs={} response_metadata={} id='run--87502fc0-37b5-46d0-9778-f0e7f794a6c9'\n",
      "content=' í•„' additional_kwargs={} response_metadata={} id='run--87502fc0-37b5-46d0-9778-f0e7f794a6c9'\n",
      "content='ìš”' additional_kwargs={} response_metadata={} id='run--87502fc0-37b5-46d0-9778-f0e7f794a6c9'\n",
      "content='í•˜' additional_kwargs={} response_metadata={} id='run--87502fc0-37b5-46d0-9778-f0e7f794a6c9'\n",
      "content='ì‹œ' additional_kwargs={} response_metadata={} id='run--87502fc0-37b5-46d0-9778-f0e7f794a6c9'\n",
      "content='ë‹¤' additional_kwargs={} response_metadata={} id='run--87502fc0-37b5-46d0-9778-f0e7f794a6c9'\n",
      "content='ë©´' additional_kwargs={} response_metadata={} id='run--87502fc0-37b5-46d0-9778-f0e7f794a6c9'\n",
      "content=' ì–¸' additional_kwargs={} response_metadata={} id='run--87502fc0-37b5-46d0-9778-f0e7f794a6c9'\n",
      "content='ì œ' additional_kwargs={} response_metadata={} id='run--87502fc0-37b5-46d0-9778-f0e7f794a6c9'\n",
      "content='ë“ ' additional_kwargs={} response_metadata={} id='run--87502fc0-37b5-46d0-9778-f0e7f794a6c9'\n",
      "content='ì§€' additional_kwargs={} response_metadata={} id='run--87502fc0-37b5-46d0-9778-f0e7f794a6c9'\n",
      "content=' ë¬¼' additional_kwargs={} response_metadata={} id='run--87502fc0-37b5-46d0-9778-f0e7f794a6c9'\n",
      "content='ì–´' additional_kwargs={} response_metadata={} id='run--87502fc0-37b5-46d0-9778-f0e7f794a6c9'\n",
      "content='ì£¼ì„¸ìš”' additional_kwargs={} response_metadata={} id='run--87502fc0-37b5-46d0-9778-f0e7f794a6c9'\n",
      "content='.' additional_kwargs={} response_metadata={} id='run--87502fc0-37b5-46d0-9778-f0e7f794a6c9'\n",
      "content=' ì¢‹' additional_kwargs={} response_metadata={} id='run--87502fc0-37b5-46d0-9778-f0e7f794a6c9'\n",
      "content='ì€' additional_kwargs={} response_metadata={} id='run--87502fc0-37b5-46d0-9778-f0e7f794a6c9'\n",
      "content=' í•˜' additional_kwargs={} response_metadata={} id='run--87502fc0-37b5-46d0-9778-f0e7f794a6c9'\n",
      "content='ë£¨' additional_kwargs={} response_metadata={} id='run--87502fc0-37b5-46d0-9778-f0e7f794a6c9'\n",
      "content=' ë˜' additional_kwargs={} response_metadata={} id='run--87502fc0-37b5-46d0-9778-f0e7f794a6c9'\n",
      "content='ì„¸ìš”' additional_kwargs={} response_metadata={} id='run--87502fc0-37b5-46d0-9778-f0e7f794a6c9'\n",
      "content='!' additional_kwargs={} response_metadata={} id='run--87502fc0-37b5-46d0-9778-f0e7f794a6c9'\n",
      "content='' additional_kwargs={} response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-3.5-turbo-0125', 'service_tier': 'default'} id='run--87502fc0-37b5-46d0-9778-f0e7f794a6c9'\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "# completion = model.invoke('ë°˜ê°€ì›Œìš”!')\n",
    "# # ì•ˆë…•í•˜ì„¸ìš”! ë§Œë‚˜ì„œ ë°˜ê°€ì›Œìš”. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?\n",
    "# print(completion)\n",
    "\n",
    "# completions = model.batch(['ë°˜ê°€ì›Œìš”!', 'ì˜ ìˆì–´ìš”!'])\n",
    "# # ['ì•ˆë…•í•˜ì„¸ìš”! ë§Œë‚˜ì„œ ë°˜ê°€ì›Œìš”! ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?', 'ê°ì‚¬í•©ë‹ˆë‹¤! ì œê°€ ì—¬ê¸° ìˆì–´ì„œ ë„ì™€ì¤„ ìˆ˜ ìˆëŠ” ì¼ì´ ìˆìœ¼ë©´ ë§ì”€í•´ì£¼ì„¸ìš”. ^^']\n",
    "# print(completions)\n",
    "\n",
    "for token in model.stream('ì˜ ìˆì–´ìš”!'):\n",
    "    print(token)\n",
    "# ì˜\n",
    "# ê°€\n",
    "# !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a0609ca-413a-46f1-80bf-fedb7d5fe9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸(LLM)ì€ í¬ê²Œ ë‘ ê°€ì§€ ë°©ì‹ìœ¼ë¡œ ì œê³µë©ë‹ˆë‹¤. \n",
      "- í´ë¼ìš°ë“œ API í˜•íƒœë¡œ ë°”ë¡œ ì‚¬ìš©í•˜ëŠ” ë°©ì‹\n",
      "- ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ì„ ë‚´ë ¤ë°›ì•„ ì§ì ‘ ìš´ì˜í•˜ëŠ” ë°©ì‹(ë˜ëŠ” ê·¸ APIë¥¼ ì´ìš©í•˜ëŠ” ë°©ì‹)\n",
      "\n",
      "ì£¼ìš” ê³µê¸‰ì²˜ì™€ ë°©ì‹ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
      "\n",
      "1) í´ë¼ìš°ë“œ API ì„œë¹„ìŠ¤\n",
      "- OpenAI API: ChatGPT, GPT-4, GPT-3.5 ë“± ë‹¤ì–‘í•œ ëª¨ë¸ì„ APIë¡œ ì œê³µ.\n",
      "- Microsoft Azure OpenAI Service: Azure í™˜ê²½ì—ì„œ OpenAI ëª¨ë¸ì„ ì—”í„°í”„ë¼ì´ì¦ˆ ìš©ë„ë¡œ ì´ìš©.\n",
      "- Google Cloud Vertex AI: PaLM ê³„ì—´ ë° Gemini ê¸°ë°˜ ëª¨ë¸ì„ API/ì„œë¹„ìŠ¤ë¡œ ì œê³µ.\n",
      "- AWS Bedrock: ì—¬ëŸ¬ íŒŒíŠ¸ë„ˆ ëª¨ë¸ì„ APIë¡œ ì´ìš©(ì˜ˆ: Anthropic Claude, AI21 Jurassic-2 ë“±) ê°€ëŠ¥.\n",
      "- Anthropic Claude, AI21 Labs Studio ë“± ê°œë³„ API: Claudeë‚˜ Jurassic-2ë¥¼ ë…ë¦½ APIë¡œë„ ì‚¬ìš© ê°€ëŠ¥.\n",
      "- (ì°¸ê³ ) í•„ìš”ì— ë”°ë¼ Cohere, Metaì˜ ê³µê°œí˜• ëª¨ë¸ ê³„ì—´ ë“±ë„ API í˜•íƒœë¡œ ì œê³µë˜ëŠ” ê²½ìš°ê°€ ìˆìŒ.\n",
      "\n",
      "2) ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ì„ ì§ì ‘ ìš´ì˜í•˜ê±°ë‚˜ APIë¡œ ì´ìš©í•˜ê¸°\n",
      "- Hugging Face Inference Endpoints: ë‹¤ì–‘í•œ ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ì„ APIë¡œ ì´ìš© ê°€ëŠ¥.\n",
      "- ì§ì ‘ ë‹¤ìš´ë¡œë“œ í›„ ìš´ì˜í•˜ëŠ” ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ë“¤: Llama 2, Mistral, Falcon, BLOOM, GPT-NeoX ë“± ë¼ì´ì„ ìŠ¤ì— ë§ì¶° ìì²´ ì„œë²„ë‚˜ í´ë¼ìš°ë“œì— ì„¤ì¹˜í•´ ì‹¤í–‰.\n",
      "  - ì´ ê²½ìš° GPUê°€ í•„ìš”í•˜ê³ , ê´€ë¦¬ì™€ ë¹„ìš©ì€ ì‚¬ìš© í™˜ê²½ì— ë”°ë¼ ë‹¬ë¼ì§‘ë‹ˆë‹¤.\n",
      "- ì˜¨-í”„ë ˜/ì—ì§€ ë°°í¬ ì˜µì…˜: ê¸°ì—…ìš©ìœ¼ë¡œ ë°ì´í„° í”„ë¼ì´ë²„ì‹œê°€ ì¤‘ìš”í•  ë•Œ í™œìš© ê°€ëŠ¥. NVIDIA NeMo, Triton ë“± ë„êµ¬ë¡œ ì„±ëŠ¥ ìµœì í™” ê°€ëŠ¥.\n",
      "\n",
      "3) ì„ íƒ ê³ ë¯¼ í¬ì¸íŠ¸\n",
      "- ë°ì´í„° í”„ë¼ì´ë²„ì‹œì™€ ê·œì • ì¤€ìˆ˜: ë¯¼ê°í•œ ë°ì´í„°ê°€ ë‹¤ë¤„ì§„ë‹¤ë©´ ì˜¨í”„ë ˜ ë˜ëŠ” í”„ë¼ì´ë¹— í´ë¼ìš°ë“œ ì˜µì…˜ì´ ìœ ë¦¬.\n",
      "- ì˜ˆì‚°ê³¼ ì‚¬ìš© ê·œëª¨: ì†Œê·œëª¨/ì´ˆê¸°ì—ëŠ” API ì‚¬ìš©ì´ í¸ë¦¬í•˜ê³  ë¹„ìš© ì˜ˆì¸¡ë„ ìš©ì´í•˜ì§€ë§Œ, ëŒ€ê·œëª¨ ì‚¬ìš©ì—ì„  ìì²´ ìš´ì˜ì´ ë” ê²½ì œì ì¼ ìˆ˜ ìˆìŒ.\n",
      "- ìµœì‹ ì„±/ë„ë©”ì¸ ì í•©ì„±: íŠ¹ì • ë¶„ì•¼ì— íŠ¹í™”ëœ ëª¨ë¸ì´ í•„ìš”í•˜ë©´ í•´ë‹¹ ë„ë©”ì¸ ëª¨ë¸(API ë˜ëŠ” ì˜¤í”ˆì†ŒìŠ¤) ì„ íƒì´ ì¤‘ìš”.\n",
      "- latencyì™€ ì§€ì—­ì„±: í•´ì™¸ ê¸°ë°˜ APIëŠ” ì§€ì—­ì— ë”°ë¼ ì‘ë‹µ ì†ë„ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ. ê·¼ì²˜ ë°ì´í„° ì„¼í„°ë¥¼ ì„ íƒí•˜ë©´ ì§€ì—°ì´ ì¤„ì–´ë“¦.\n",
      "\n",
      "ì›í•˜ëŠ” ìš©ë„ë‚˜ ì˜ˆì‚°, ë°ì´í„° í”„ë¼ì´ë²„ì‹œ ìš”ê±´ì´ ìˆë‚˜ìš”? ëª©ì ì— ë§ì¶° êµ¬ì²´ì ì¸ ì¶”ì²œ(ì„œë¹„ìŠ¤ ë¹„êµ, ì‚¬ìš© ë°©ë²•, ì˜ˆì‚° ì¶”ì •)ì„ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import chain\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# êµ¬ì„± ìš”ì†Œ\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system', 'ë‹¹ì‹ ì€ ì¹œì ˆí•œ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.'),\n",
    "        ('human', '{question}'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = ChatOpenAI(model='gpt-5-nano')\n",
    "\n",
    "# í•¨ìˆ˜ë¡œ ê²°í•©í•œë‹¤\n",
    "# ë°ì½”ë ˆì´í„° @chainì„ ì¶”ê°€í•´ ì‘ì„±í•œ í•¨ìˆ˜ì— Runnable ì¸í„°í˜ì´ìŠ¤ë¥¼ ì¶”ê°€í•œë‹¤\n",
    "@chain\n",
    "def chatbot(values):\n",
    "    prompt = template.invoke(values)\n",
    "    return model.invoke(prompt)\n",
    "\n",
    "# ì‚¬ìš©í•œë‹¤\n",
    "response = chatbot.invoke({'question': 'ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸ì€ ì–´ë””ì„œ ì œê³µí•˜ë‚˜ìš”?'})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e93158-ecbc-480f-9703-701833469322",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import chain\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "@chain\n",
    "def chatbot(values):\n",
    "    prompt = template.invoke(values)\n",
    "    for token in model.stream(prompt):\n",
    "        yield token\n",
    "\n",
    "for part in chatbot.stream({\n",
    "    'question': 'ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸ì€ ì–´ë””ì„œ ì œê³µí•˜ë‚˜ìš”?'\n",
    "}):\n",
    "    print(part)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "84675783-8660-4c6d-a942-14319a2c01c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸ì€ ë‹¤ì–‘í•œ íšŒì‚¬ì™€ ì—°êµ¬ ê¸°ê´€ì—ì„œ ì œê³µë©ë‹ˆë‹¤. ê°€ì¥ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸ ì¤‘ ì¼ë¶€ëŠ” OpenAIì˜ GPT ì‹œë¦¬ì¦ˆ, Googleì˜ BERT ë° T5, í˜ì´ìŠ¤ë¶ì˜ RoBERTa ë“±ì´ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ëª¨ë¸ì€ ëŒ€ë¶€ë¶„ ì˜¤í”ˆ ì†ŒìŠ¤ë¡œ ê³µê°œë˜ì–´ ìˆì–´ ëˆ„êµ¬ë‚˜ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìš”êµ¬ ì‚¬í•­ì´ë‚˜ ì‚¬ìš© ë°©ë²•ì€ ê° ëª¨ë¸ì˜ ê³µì‹ ì›¹ì‚¬ì´íŠ¸ë‚˜ ë¬¸ì„œì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 153, 'prompt_tokens': 36, 'total_tokens': 189, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CMIXtEea7Ybl2dOwu5bYVRhzTZiuV', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--7bffeba9-a305-4bc5-bc49-ba1436e70b36-0', usage_metadata={'input_tokens': 36, 'output_tokens': 153, 'total_tokens': 189, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import chain\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "\n",
    "@chain\n",
    "async def chatbot(values):\n",
    "    prompt = await template.ainvoke(values)\n",
    "    return await model.ainvoke(prompt)\n",
    "\n",
    "\n",
    "await chatbot.ainvoke({'question': 'ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸ì€ ì–´ë””ì„œ ì œê³µí•˜ë‚˜ìš”?'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "44a07791-f5f6-4a34-a1ad-da5248ddbd12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸ì€ ì—¬ëŸ¬ íšŒì‚¬ì™€ ì—°êµ¬ ê¸°ê´€ì—ì„œ ê°œë°œë˜ê³  ì œê³µë©ë‹ˆë‹¤. ëª‡ ê°€ì§€ ì£¼ìš” ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸ì˜ ì˜ˆì‹œëŠ” OpenAIì˜ GPT ì‹œë¦¬ì¦ˆ, Googleì˜ BERT ë° T5, Microsoftì˜ Turing ë“±ì´ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ëª¨ë¸ì€ í•´ë‹¹ ì¡°ì§ì˜ ì›¹ì‚¬ì´íŠ¸ë‚˜ í”Œë«í¼ì„ í†µí•´ ì œê³µë˜ê±°ë‚˜ ì˜¤í”ˆì†ŒìŠ¤ë¡œ ê³µê°œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, OpenAIëŠ” ê³µê°œ ê³µí‘œëœ API ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í†µí•´ GPT-3 ëª¨ë¸ì„ ì œê³µí•©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# êµ¬ì„± ìš”ì†Œ\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system', 'ë‹¹ì‹ ì€ ì¹œì ˆí•œ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.'),\n",
    "        ('human', '{question}'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "# ì—°ì‚°ì | ë¡œ ê²°í•©í•œë‹¤\n",
    "\n",
    "chatbot = template | model\n",
    "\n",
    "# ì‚¬ìš©í•œë‹¤\n",
    "\n",
    "response = chatbot.invoke({'question': 'ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸ì€ ì–´ë””ì„œ ì œê³µí•˜ë‚˜ìš”?'})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ed5239-3c74-4edc-8d46-1d0c0d5aa26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "loader = TextLoader('./test.txt', encoding='utf-8')\n",
    "docs = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splitted_docs = splitter.split_documents(docs)\n",
    "\n",
    "print(splitted_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1879e8-3556-4aa3-9374-48be9dd02852",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "model = OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "embeddings = model.embed_documents([\n",
    "    'Hi there!',\n",
    "    'Oh, hello!',\n",
    "    'What\\'s your name?',\n",
    "    'My friends call me World',\n",
    "    'Hello World!'\n",
    "])\n",
    "\n",
    "print(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f88745-babf-4e0c-8b1c-4464606b1313",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
